{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import GridSearchCV, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Paths\n",
    "BASE_DIR = Path(\"../../data/preprocessed\")\n",
    "SAVE_DIR = Path(\"../../results\")\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Paths for different join types, scaling methods, and transformations\n",
    "JOIN_TYPES = ['outer', 'left', 'right']\n",
    "SCALING_METHODS = ['standard', 'minmax']\n",
    "TRANSFORMATIONS = ['original', 'log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset names and their corresponding filenames\n",
    "datasets = {}\n",
    "for join_type in JOIN_TYPES:\n",
    "    for scaling_method in SCALING_METHODS:\n",
    "        for transformation in TRANSFORMATIONS:\n",
    "            dataset_name = f\"{join_type}_{scaling_method}_{transformation}\"\n",
    "            train_file = BASE_DIR / join_type / scaling_method / transformation / \"train_data_scaled.csv\"\n",
    "            test_file = BASE_DIR / join_type / scaling_method / transformation / \"test_data_scaled.csv\"\n",
    "            datasets[dataset_name] = (train_file, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UmrK0vMLopoR</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CPaeQkhcjg7d</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nb4EetVPm3gs</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p4vPhVu91o4b</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>M09PXs7arQ5E</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ADHD_Outcome  Sex_F\n",
       "participant_id                     \n",
       "UmrK0vMLopoR               1      1\n",
       "CPaeQkhcjg7d               1      0\n",
       "Nb4EetVPm3gs               1      0\n",
       "p4vPhVu91o4b               1      1\n",
       "M09PXs7arQ5E               1      1"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load target labels\n",
    "y_train = pd.read_excel(\"../../data/raw/widsdatathon2025/TRAIN/TRAINING_SOLUTIONS.xlsx\")\n",
    "y_train = y_train.set_index(\"participant_id\")\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_file, test_file):\n",
    "    \"\"\"Load training and test datasets.\"\"\"\n",
    "    # Create X_train and X_test\n",
    "    X_train = pd.read_csv(train_file)\n",
    "    X_test = pd.read_csv(test_file)\n",
    "\n",
    "    # Extract participant IDs and set index\n",
    "    participant_id = X_test[\"participant_id\"]\n",
    "    X_train = X_train.set_index(\"participant_id\")\n",
    "    X_test = X_test.set_index(\"participant_id\")\n",
    "\n",
    "    return X_train, X_test, participant_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(X_train, y_train):\n",
    "#     \"\"\"Train an XGBoost MultiOutputClassifier model with GridSearchCV.\"\"\"\n",
    "#     # Define the base XGBoost classifier\n",
    "#     xgb_classifier = XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "#     # Wrap in MultiOutputClassifier\n",
    "#     multioutput_classifier = MultiOutputClassifier(xgb_classifier)\n",
    "\n",
    "#     # Define hyperparameters for GridSearchCV\n",
    "#     param_grid = {\n",
    "#         'estimator__n_estimators': [100, 200, 300],\n",
    "#         'estimator__learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'estimator__max_depth': [3, 5, 7],\n",
    "#         'estimator__colsample_bytree': [0.7, 0.9, 1.0],\n",
    "#         'estimator__subsample': [0.7, 0.9, 1.0],\n",
    "#         'estimator__gamma': [0, 0.1, 0.2],\n",
    "#         'estimator__reg_lambda': [0, 1, 10],\n",
    "#         'estimator__reg_alpha': [0, 1, 10],\n",
    "#     }\n",
    "\n",
    "#     # Define KFold cross-validation\n",
    "#     kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     # Perform GridSearchCV\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=multioutput_classifier,\n",
    "#         param_grid=param_grid,\n",
    "#         cv=kfold,\n",
    "#         scoring='accuracy',\n",
    "#         n_jobs=-1,\n",
    "#         verbose=1\n",
    "#     )\n",
    "\n",
    "#     # Fit the model\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "\n",
    "#     # Print the best parameters\n",
    "#     print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "\n",
    "#     return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train):\n",
    "    \"\"\"Train an XGBoost MultiOutputClassifier model without GridSearchCV.\"\"\"\n",
    "    # Define the base XGBoost classifier with chosen hyperparameters\n",
    "    xgb_classifier = XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        colsample_bytree=0.9,\n",
    "        subsample=0.9,\n",
    "        gamma=0.1,\n",
    "        reg_lambda=1,\n",
    "        reg_alpha=1\n",
    "    )\n",
    "\n",
    "    # Wrap in MultiOutputClassifier\n",
    "    multioutput_classifier = MultiOutputClassifier(xgb_classifier)\n",
    "\n",
    "    # Fit the model directly\n",
    "    multioutput_classifier.fit(X_train, y_train)\n",
    "\n",
    "    return multioutput_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, X_test, participant_id, dataset_name):\n",
    "    \"\"\"Generate predictions and save results.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Convert predictions to a DataFrame\n",
    "    predictions_df = pd.DataFrame(\n",
    "        y_pred, columns=['Predicted_Gender', 'Predicted_ADHD'], index=X_test.index\n",
    "    )\n",
    "\n",
    "    result_df = predictions_df.reset_index()\n",
    "\n",
    "    # Save the results\n",
    "    result_file = SAVE_DIR / f\"{dataset_name}_xgb.csv\"\n",
    "    result_df.to_csv(result_file, index=False)\n",
    "\n",
    "    print(f\"Results saved to {result_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: outer_standard_original\n",
      "Results saved to ../../results/outer_standard_original_xgb.csv\n",
      "Processing dataset: outer_standard_log\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/preprocessed/outer/standard/log/train_data_scaled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing dataset: \u001b[39m\u001b[39m{\u001b[39;00mdataset_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Load data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m X_train, X_test, participant_id \u001b[39m=\u001b[39m load_data(train_file, test_file)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Train model \u001b[39;00m\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m train_model(X_train, y_train)\n",
      "Cell \u001b[0;32mIn[54], line 4\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(train_file, test_file)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load training and test datasets.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Create X_train and X_test\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m X_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(train_file)\n\u001b[1;32m      5\u001b[0m X_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(test_file)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Extract participant IDs and set index\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/WiDS-Datathon-Team-25/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Desktop/WiDS-Datathon-Team-25/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Desktop/WiDS-Datathon-Team-25/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Desktop/WiDS-Datathon-Team-25/venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Desktop/WiDS-Datathon-Team-25/venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/preprocessed/outer/standard/log/train_data_scaled.csv'"
     ]
    }
   ],
   "source": [
    "for dataset_name, (train_file, test_file) in datasets.items():\n",
    "    print(f\"Processing dataset: {dataset_name}\")\n",
    "\n",
    "    # Load data\n",
    "    X_train, X_test, participant_id = load_data(train_file, test_file)\n",
    "\n",
    "    # Train model \n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    # Predict and save results\n",
    "    predict_and_save(model, X_test, participant_id, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
